---
title: "ICE Corpus"
output: html_notebook
---


# Intro

- About ICE corpus
- About this notebook

# Set up?

For the code in this notebook to run, we need to install and load the packages used. First, we install them using the install.packages function.

```{r}
install.packages("tidyverse")
install.packages("quanteda")
install.packages("quanteda.textstats")
```

Now that they have been installed, we need to load the packages using the library function.

```{r}
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
```


# Download the data

- Download data from web


```{r}
# work out easiest way to download data
# ice_corpus <- 
```

# Processing the data

```{r}
# get list of files
corpus_files <- list.files("data", recursive = TRUE, pattern = "*.TXT", full.names = TRUE)
# define function to load all files
load_file <- function(input_file){
  paste(readLines(input_file), collapse="\n")
}
# apply function to all corpus files
loaded_texts <- sapply(corpus_files, load_file)
# to do: suppress incomplete final line warning
# bring in communication mode (written or spoken)
extract_communication_mode <- function(input_file){
  substring(basename(input_file),1,1)
}
# apply to all files
communication_mode <- sapply(corpus_files, extract_communication_mode)
# bring in subcorpora
extract_subcorpus <- function(input_file){
  substring(basename(input_file),1,3)
}
# apply to all files
subcorpus <- sapply(corpus_files, extract_subcorpus)
```

# Analysis

Now that we have the data loaded, we can start running some analysis steps. First, we need to... 

- Run analysis (keywords) on data as is
- Discuss why it's weird (markup)

```{r}
# create corpus object from loaded texts using corpus function
ice_corpus <- corpus(loaded_texts)
# add document variables (communication mode and subcorpus) --- can probably combine these?
docvars(ice_corpus, field = "communication_mode") <- communication_mode
docvars(ice_corpus, field = "subcorpus") <- subcorpus
# tokenise corpus
ice_tokens <- tokens(ice_corpus)
# kwic(ice_tokens, pattern = "banana")
# create document feature matrix for analysis
ice_dfm <- dfm(ice_tokens)
```

## Keyword Analysis

First, we will run keyword analysis, which is a commonly used text analysis technique. The function textstats_keyness identifies words that are significantly more frequent in a target text than a reference text, and therefore can be said to be characteristic of the target text in some way. (rewrite) 

Here, we will be using the document variables that we added in the previous section to compare all spoken texts against all written texts, to identify keywords of each.

```{r}
# run keyword analysis comparing all spoken files to written files
textstat_keyness(ice_dfm, target = docvars(ice_dfm, "communication_mode")=="S")
# run keyword analysis comparing all written files to spoken files
textstat_keyness(ice_dfm, target = docvars(ice_dfm, "communication_mode")=="W")
```

As you can see, the results contain a lot of unexpected symbols. If we inspect the data, we can see that it contains markup as part of the text:

```{r}
head(ice_corpus)
```
Likely, what we are seeing in the keyword analysis above is merely the difference between the written and spoken markup conventions. In order to get a more accurate keyword list, we should remove the markup first, and rerun the keyword analysis on the bare text.

- results above contain a lot of unexpected symbols (markup)
- comparison between written and spoken files probably just showing difference in written and spoken markup (eg, inaudible)

# Processing the data (take two)

So, we need to do some additional data processing. We can't just go ahead and remove anything that isn't regular text. We need to carefully inspect the data (and metadata) to work out what we want to keep and what we want to get rid of. There isn't necessarily a right or wrong approach here - it will depend on your particular research. Depending on what you are interested in, it might be very important to retain some parts of the markup. As the main analytic method we are using here is keyword analysis, which looks only at word frequencies, we can confidently remove all markup and proceed with just the bare text.

## Removing Markup

The first problem we ran into with this data was that there is no explanation of the markup in the metadata files. We did manage to find markup manuals on the International Corpus of English website: https://www.ice-corpora.uzh.ch/en/manuals.html

On first glance, the markup described in these manuals does appear to match the markup in our data. That said, it's best to closely inspect the data to be sure. Rather than reading through the entire dataset to make sure all markup is consistent with the manuals, let's create an inventory of all markup tags and symbols present. 

```{r}
combined_texts <- str_c(loaded_texts)
markup_inventory <- function(input_file){
  input_file %>%
    str_replace_all("<", " <") %>%
    str_replace_all(">", "> ") %>%
    str_replace_all(" ", "\n") %>%
    unique() %>% # this doesn't seem to be doing anything
    str_extract_all(".*[\\<\\>].*")
}
inventory <- markup_inventory(combined_texts)
head(inventory)
```

If we carefully inspect this our markup inventory, we can see that the tags mostly match the markup manuals, with a couple of exceptions. First, the 'text unit marker' tag contains a back slash in our data ("#\\>" - this shows up as a double back slash in R as it is a special character that has to be escaped). Second, there is an extra tag: <*>. If we take a closer look at this tag in context, we can see that it seems to signify text formatting:

```{r}
str_extract_all(combined_texts, ".*\\<.\\*\\>.*") # is there a way to get this to only display matches?
```

Now that we know how the tags are actually used in our data, we need to decide what to keep and what to get rid of. We can see from the manual and from our inventory, that some tags are used in pairs with additional markup in between (e.g., "<O>inaudible</O>"). If we had simply removed all markup, we would have ended up with markup text incorrectly included in our analysis. There is also some markup text that we probably do want to include, such as overlapping strings, so we also can't just remove anything between tags.

It's important to note that decisions around what to include and what to remove are part of your analysis. This is not just data cleaning. These choices aren't necessarily clear and they depend on your research question and aims. For our purposes, we are going to remove any untranscribed text ("<O>...</O>"), extra-corpus text ("<X>...</X>"), formatting text ("<*>...</*>"), and editorial comments ("<&>...</&>"). Then, we will remove all markup sybols. 

- Discussion of data processing/'cleaning', what decisions are made here (depends what you're looking at)
- Actually look at data (inventory of tags), link to ICE markup guide (but point out that it might not be consistent - again, look at data!)
- Decide what to remove


```{r}
# Deal with markup
# Replace new line with spaces, remove &*OX tags, put new lines back in before speaker/new paragraph tag (diff for spoken vs written), remove all markup
#remove_markup <- function(input_file){
  
#}
#str_remove_all with regex? spoken and written markup different?
# str_remove_all(loaded_texts[1], "<[^<>]*>") - might need multiple rounds of regex to capture everything
# test <- str_replace_all(loaded_texts[1], "\n", " ")
# test2 <- str_remove_all(test, "<O>.*?</O>")
# test3 <- str_remove_all(test2, "<X>.*?</X>")
# test4 <- str_remove_all(test3, "<&>.*?</&>")
# put line breaks back in
# why is <#\> turning into <#\\> ???? need to escape both 
# test5 <- str_replace_all(test4, "<#\\\\>", "\n")
# test6 <- str_remove_all(test5, "<.*?>")
# check written 
# writtentest <- load_file("data/ICE Written/W1A/W1A-001.TXT")
# writtentest1 <- str_replace_all(writtentest, "\n", " ")
#wtest2 <- str_remove_all(writtentest1, "<O>.*?</O>")
#wtest3 <- str_remove_all(wtest2, "<X>.*?</X>")
#wtest4 <- str_remove_all(wtest3, "<&>.*?</&>")
# put line breaks back in
# why is <#\> turning into <#\\> ???? need to escape both 
#wtest5 <- str_replace_all(wtest4, "<#\\\\>", "\n")
#wtest6 <- str_remove_all(wtest5, "<.*?>")
# define function to remove markup
remove_markup <- function(input_file){
  input_file %>%
    str_replace_all("\n", " ") %>%
    str_remove_all("<O>.*?</O>") %>%
    str_remove_all("<X>.*?</X>") %>%
    str_remove_all("<&>.*?</&>") %>%
    str_remove_all("<\\*>.*?</\\*>") %>%
    str_replace_all("<#\\\\>", "\n") %>%
    str_remove_all("<.*?>")
}
# apply function to all files
loaded_texts_no_markup <- sapply(loaded_texts, remove_markup)
# testfunction <- remove_markup(loaded_texts[1])
```
# Analysis (take two)

- Look it's different now

```{r}
# create corpus (without markup)
ice_corpus_2 <- corpus(loaded_texts_no_markup)
# add document variables
docvars(ice_corpus_2, field = "communication_mode") <- communication_mode
docvars(ice_corpus_2, field = "subcorpus") <- subcorpus
# tokenise
ice_tokens_2 <- tokens(ice_corpus_2)
# kwic(ice_tokens, pattern = "banana")
# create new dfm without markup
ice_dfm_2 <- dfm(ice_tokens_2)
# punctuation??
```

- now when we run analysis again we shouldn't get all those markup symbols

## Keyword Analysis

```{r}
# rerun keyword analysis without markup 
textstat_keyness(ice_dfm_2, target = docvars(ice_dfm_2, "communication_mode")=="S")
textstat_keyness(ice_dfm_2, target = docvars(ice_dfm_2, "communication_mode")=="W")
```
- now we see some true differences (e.g., lots of filler words in spoken, lots of punctuation in written -- punctuation is also something that should be considered, do we delete? depends)
- statistics??
- how many texts actually use keywords? (below)
- compare between subcorpora (below)
- kwic for specific things (below)
- numbers -  seem to be written in full in spoken and numerals in written, not consistent though 
- tonight/today?
- have to go much further down to see things we talk about but don't write about, compared to things we write about but don't talk about

## Keywords in Context

```{r}
# context for keywords
kwic(ice_tokens_2, "faithfully")
kwic(ice_tokens_2, "wombats")
kwic(ice_tokens_2, "echidnas")
kwic(ice_tokens_2, "worship")
kwic(ice_tokens_2, "ladies")
kwic(ice_tokens_2, "galaxy")
```

- some keywords (e.g., above) appear many times in a few files (wombats in W2B-021, echidnas in W2B-027 - files are from nature magazines)
- check metadata for what these files are (e.g., 'faithfully' is almost always after 'yours', ie as a sign off to a letter)
- context can be crucial - 'worship' is a keyword in spoken files which initially seems odd, but in context it's clear that it is almost exclusively used as a title (Your Worship) --- this is also interesting because it's an obsolete title in australia (add reference)
- galaxy surprising keyword for spoken - almost exclusively in one file (astronomy broadcast)

## Subcorpus Comparison

```{r}
# subcorpora vs whole corpus
textstat_keyness(ice_dfm_2, target = docvars(ice_dfm_2, "subcorpus")=="W1A")
textstat_keyness(ice_dfm_2, target = docvars(ice_dfm_2, "subcorpus")=="S1A")
```
- some of these end up looking very similar to the earlier comparison between written and spoken (eg S1A above)
- this might change if we compare only to other spoken files

```{r}
# run keyword analysis on subcorpora
written_dfm <- dfm_subset(ice_dfm_2, subset = ice_dfm_2$communication_mode == "W")
spoken_dfm <- dfm_subset(ice_dfm_2, subset = ice_dfm_2$communication_mode == "S")
textstat_keyness(written_dfm, target = docvars(written_dfm, "subcorpus")=="W1A")
textstat_keyness(spoken_dfm, target = docvars(spoken_dfm, "subcorpus")=="S1A")
```
- keywords actually don't change, which is probably because S1A is dialogues, so it would be expected to have the strongest features of spoken language
- stats do change though 

```{r}
textstat_keyness(ice_dfm_2, target = docvars(ice_dfm_2, "subcorpus")=="S2B")
textstat_keyness(spoken_dfm, target = docvars(spoken_dfm, "subcorpus")=="S2B")
```
- much more of a difference here

```{r}
textstat_keyness(ice_dfm_2, target = docvars(ice_dfm_2, "subcorpus")=="W2F")
textstat_keyness(written_dfm, target = docvars(written_dfm, "subcorpus")=="W2F")
```
- W2F is novels, so a lot of keywords seem to be character names
- first person pronouns are much higher in the comparison to other written files (they are more common in spoken language than written)
